# -*- coding: utf-8 -*-
"""Machine Learning based Classification for Sentimental Analysis of IMDb Reviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JLEQgIwFtJ34C8yeneWnflqJipAgTHPB

# Importing Libraries
"""

import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import keras
import random
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
import re
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
import nltk
nltk.download('wordnet')
import os

"""# Data loading"""

train_data, validation_data, test_data = tfds.load(
    name="imdb_reviews",
    split=('train[:80%]', 'train[20%:]', 'test'),
    as_supervised=True)

"""# N-gram model

## Binary, Word_count, Tfidf
"""

path = '/content/drive/MyDrive/CS229/Final/movie_data/full_train.txt'
reviews_train = []
for line in open(path, 'r'):
    reviews_train.append(line.strip())

path = '/content/drive/MyDrive/CS229/Final/movie_data/full_test.txt'
reviews_test = []
for line in open(path, 'r'):
    reviews_test.append(line.strip())

train_label = [1 if i < 12500 else 0 for i in range(25000)]
test_label = [1 if i < 12500 else 0 for i in range(25000)]

X_train, X_val, y_train, y_val = train_test_split(
    reviews_train, train_label, train_size=0.8
)

REPLACE_NO_SPACE = re.compile("[.;:!\'?,\"()\[\]]")
REPLACE_WITH_SPACE = re.compile("(<br\s*/><br\s*/>)|(\-)|(\/)")
def preprocess_reviews(reviews):
  reviews = [REPLACE_NO_SPACE.sub("", line.lower()) for line in reviews]
  reviews = [REPLACE_WITH_SPACE.sub(" ", line) for line in reviews]
  return reviews

reviews_train_clean = preprocess_reviews(X_train)
reviews_val_clean = preprocess_reviews(X_val)
reviews_test_clean = preprocess_reviews(reviews_test)

def get_lemmatized_text(corpus):
  lemmatizer = WordNetLemmatizer()
  return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]

reviews_train_clean = get_lemmatized_text(reviews_train_clean)
reviews_val_clean = get_lemmatized_text(reviews_val_clean)
reviews_test_clean = get_lemmatized_text(reviews_test_clean)

models = []
models.append(('Logistic', LogisticRegression(C=0.05)))
models.append(('RF', RandomForestClassifier()))
models.append(('Boosting', GradientBoostingClassifier(max_depth= 5)))
models.append(('SVC', LinearSVC(C=0.05)))
# models.append(('DNN', MLPClassifier(hidden_layer_sizes=(30,30,20,10,10), activation='logistic', early_stopping=True)))
stop_words = ['in', 'of', 'at', 'a', 'the']
preprocessing = []
preprocessing.append(('Binary', CountVectorizer(binary=True, stop_words=stop_words)))
preprocessing.append(('Count', CountVectorizer(binary=False, ngram_range=(1,3), stop_words=stop_words)))
preprocessing.append(('Tf-idf', TfidfVectorizer(ngram_range=(1, 3), stop_words=stop_words)))
# evaluate each model in turn
val_accuracy = []
test_accuracy = []
names = []
for preprocess_name, preprocess_method in preprocessing:
  preprocess_method = preprocess_method.fit(reviews_train_clean)
  X_train = preprocess_method.transform(reviews_train_clean)
  X_val = preprocess_method.transform(reviews_val_clean)
  X_test = preprocess_method.transform(reviews_test_clean)
  for name, model in models:
    model.fit(X_train, y_train)
    val_label = model.predict(X_val)
    val_accuracy.append(accuracy_score(val_label, y_val))
    y_pred = model.predict(X_test)
    test_accuracy.append(accuracy_score(test_label, y_pred))
    names.append(name + '-' + preprocess_name)
    print('%s val_accuracy: %f, test_accuracy: %f' % (name + '-' + preprocess_name, accuracy_score(val_label, y_val), accuracy_score(test_label, y_pred)))

"""### Performance"""

x = np.arange(len(names))

width = 0.3
plt.figure(figsize = (20, 10))
plt.bar(x - 0.17, val_accuracy, width, label='Validation')
plt.bar(x + 0.17, test_accuracy, width, label='Test')
plt.xticks(ticks=x, labels=names,
           rotation=45)
for index, value in enumerate(val_accuracy):
    plt.text(index - 0.4, value+0.01, str(value)[0:4])
for index, value in enumerate(test_accuracy):
    plt.text(index + 0.04, value + 0.01, str(value)[0:4])
plt.ylabel(f'Accuracy for different models')
_ = plt.legend()

"""## Word Embedding

Use three embedding methods, NNLM 50, NNLM 128, NNLM 128 with normalization and universal encoder
"""

class dense_embedding(tf.keras.Model):

  def __init__(self, input):
    super(dense_embedding, self).__init__()
    self.hub_layer = hub.KerasLayer(input, input_shape=[],
                           dtype=tf.string, trainable=True)
    self.dense1 = tf.keras.layers.Dense(64, activation='relu')
    self.dense2 = tf.keras.layers.Dense(32, activation='relu')
    self.res = tf.keras.layers.Dense(1, activation='sigmoid')

  def call(self, x):
    x = self.hub_layer(x)
    x = self.dense1(x)
    x = self.dense2(x)
    return self.res(x)

embedding_50 = "https://tfhub.dev/google/nnlm-en-dim50/2"
model = dense_embedding(embedding_50)

MAX_EPOCHS = 30

def compile_and_fit(model, patience=2):
  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                    patience=patience,
                                                    mode='min')
  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-3,
    decay_steps=500,
    decay_rate=0.9)
  model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

  history = model.fit(train_data.batch(512), epochs=MAX_EPOCHS,
                      validation_data=validation_data.batch(512))
                      # ,callbacks=[early_stopping])
  return history

embedding_50 = "https://tfhub.dev/google/nnlm-en-dim50/2"
hub_layer = hub.KerasLayer(embedding_50, input_shape=[],
                           dtype=tf.string, trainable=True)


model50 = tf.keras.Sequential()
model50.add(hub_layer)
model50.add(tf.keras.layers.Dense(64, activation='relu'))
model50.add(tf.keras.layers.Dense(32, activation='relu'))
model50.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model50.summary()

history = compile_and_fit(model50)

performance = {}
val_performance = {}
val_performance['nlnm 50'] = model50.evaluate(validation_data.batch(512))
performance['nlnm 50'] = model50.evaluate(test_data.batch(512))

embedding_50 = "https://tfhub.dev/google/nnlm-en-dim128/2"
hub_layer = hub.KerasLayer(embedding_50, input_shape=[],
                           dtype=tf.string, trainable=True)


model128 = tf.keras.Sequential()
model128.add(hub_layer)
model128.add(tf.keras.layers.Dense(64, activation='relu'))
model128.add(tf.keras.layers.Dense(32, activation='relu'))
model128.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model128.summary()

history = compile_and_fit(model128)

val_performance['nlnm 128'] = model128.evaluate(validation_data.batch(512))
performance['nlnm 128'] = model128.evaluate(test_data.batch(512))

embedding_50 = "https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2"
hub_layer = hub.KerasLayer(embedding_50, input_shape=[],
                           dtype=tf.string, trainable=True)


model128_norm = tf.keras.Sequential()
model128_norm.add(hub_layer)
model128_norm.add(tf.keras.layers.Dense(64, activation='relu'))
model128_norm.add(tf.keras.layers.Dense(32, activation='relu'))
model128_norm.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model128_norm.summary()

history = compile_and_fit(model128_norm)

val_performance['nlnm 128_norm'] = model128_norm.evaluate(validation_data.batch(512))
performance['nlnm 128_norm'] = model128_norm.evaluate(test_data.batch(512))

embedding_50 = "https://tfhub.dev/google/universal-sentence-encoder/4"
hub_layer = hub.KerasLayer(embedding_50, input_shape=[],
                           dtype=tf.string, trainable=True)


model512 = tf.keras.Sequential()
model512.add(hub_layer)
model512.add(tf.keras.layers.Dense(64, activation='relu'))
model512.add(tf.keras.layers.Dense(32, activation='relu'))
model512.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model512.summary()

history = compile_and_fit(model512)

val_performance['dan 512'] = model512.evaluate(validation_data.batch(512))
performance['dan 512'] = model512.evaluate(test_data.batch(512))

"""### Comparison

Make the embedding layers trainable improve the performance.
"""

x = np.arange(len(performance))
width = 0.3

metric_name = 'accuracy'
metric_index = model512.metrics_names.index('accuracy')
val_mae = [v[metric_index] for v in val_performance.values()]
test_mae = [v[metric_index] for v in performance.values()]

plt.figure(figsize = (20, 10))
plt.bar(x - 0.17, val_mae, width, label='Validation')
plt.bar(x + 0.17, test_mae, width, label='Test')
plt.xticks(ticks=x, labels=performance.keys(),
           rotation=45)
for index, value in enumerate(val_mae):
    plt.text(index - 0.4, value+0.01, str(value)[0:4])
for index, value in enumerate(test_mae):
    plt.text(index + 0.04, value + 0.01, str(value)[0:4])
plt.ylabel('Accuracy (average over all outputs)')

_ = plt.legend()

"""# Sequence model

## RNN

### Text Encoder
"""

VOCAB_SIZE = 1000
encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(
    max_tokens=VOCAB_SIZE)
encoder.adapt(train_data.map(lambda text, label: text))

"""### Model"""

model_lstm = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(
        input_dim=len(encoder.get_vocabulary()),
        output_dim=64,
        # Use masking to handle the variable sequence lengths
        mask_zero=True),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])

history = compile_and_fit(model_lstm)

val_performance={}
performance={}
val_performance['LSTM'] = model_lstm.evaluate(validation_data.batch(512))
performance['LSTM'] = model_lstm.evaluate(test_data.batch(512))

model_doubleLSTM = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 100, mask_zero=True),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1)
])

history = compile_and_fit(model_doubleLSTM)

val_performance['Double LSTM'] = model_doubleLSTM.evaluate(validation_data.batch(512))
performance['Double LSTM'] = model_doubleLSTM.evaluate(test_data.batch(512))

"""### Model with pretrained embedding layer"""

# !wget http://nlp.stanford.edu/data/glove.6B.zip
# !unzip -q glove.6B.zip

# convert encoder to dict (word as key index as value)
word_index = dict(zip(encoder.get_vocabulary(), range(1000)))

# convert txt to dictionary (word as key, coefs as value)
path_to_glove_file = '/content/glove.6B.100d.txt'

embeddings_index = {}
with open(path_to_glove_file) as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, "f", sep=" ")
        embeddings_index[word] = coefs

print("Found %s word vectors." % len(embeddings_index))

# integrate the above two dict to create a embedding matrix for the encoder of the model
num_tokens = 1000
embedding_dim = 100
hits = 0
misses = 0

# Prepare embedding matrix
embedding_matrix = np.zeros((num_tokens, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Words not found in embedding index will be all-zeros.
        # This includes the representation for "padding" and "OOV"
        embedding_matrix[i] = embedding_vector
        hits += 1
    else:
        misses += 1
print("Converted %d words (%d misses)" % (hits, misses))

model_lstm_pretrained = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(
        input_dim=len(encoder.get_vocabulary()),
        output_dim=100,
        embeddings_initializer = keras.initializers.Constant(embedding_matrix),
        trainable = True,
        # Use masking to handle the variable sequence lengths
        mask_zero=True),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])

history = compile_and_fit(model_lstm_pretrained)

val_performance['LSTM_pretrain'] = model_lstm_pretrained.evaluate(validation_data.batch(512))
performance['LSTM_pretrain'] = model_lstm_pretrained.evaluate(test_data.batch(512))

model_doubleLSTM_pretrain = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(
        input_dim=len(encoder.get_vocabulary()),
        output_dim=100,
        embeddings_initializer = keras.initializers.Constant(embedding_matrix),
        trainable = True,
        # Use masking to handle the variable sequence lengths
        mask_zero=True),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1)
])

history = compile_and_fit(model_doubleLSTM_pretrain)

val_performance['Double LSTM_pretrain'] = model_doubleLSTM_pretrain.evaluate(validation_data.batch(512))
performance['Double LSTM_pretrain'] = model_doubleLSTM_pretrain.evaluate(test_data.batch(512))

x = np.arange(len(performance))
width = 0.3

metric_name = 'accuracy'
metric_index = model_doubleLSTM.metrics_names.index('accuracy')
val_mae = [v[metric_index] for v in val_performance.values()]
test_mae = [v[metric_index] for v in performance.values()]

plt.figure(figsize = (20, 10))
plt.ylabel('Accuracy (average over all outputs)')
plt.bar(x - 0.17, val_mae, width, label='Validation')
plt.bar(x + 0.17, test_mae, width, label='Test')
plt.xticks(ticks=x, labels=performance.keys(),
           rotation=45)
for index, value in enumerate(val_mae):
    plt.text(index - 0.4, value+0.01, str(value)[0:4])
for index, value in enumerate(test_mae):
    plt.text(index + 0.04, value + 0.01, str(value)[0:4])


_ = plt.legend()

"""## BERT"""

# !pip install tensorflow_text
import tensorflow_text as text  # Registers the ops.

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  preprocessing_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3", name='preprocessing')
  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1', trainable=True, name='BERT_encoder')
  outputs = encoder(encoder_inputs)
  net = outputs['pooled_output']
  net = tf.keras.layers.Dropout(0.1)(net)
  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
  return tf.keras.Model(text_input, net)

model_Bert = build_classifier_model()
history = compile_and_fit(model_Bert)

val_performance['BERT'] = model_Bert.evaluate(validation_data.batch(512))
performance['BERT'] = model_Bert.evaluate(test_data.batch(512))

x = np.arange(len(performance))
width = 0.3

metric_name = 'accuracy'
metric_index = model512.metrics_names.index('accuracy')
val_mae = [v[metric_index] for v in val_performance.values()]
test_mae = [v[metric_index] for v in performance.values()]

plt.ylabel('Accuracy (average over all outputs)')
plt.bar(x - 0.17, val_mae, width, label='Validation')
plt.bar(x + 0.17, test_mae, width, label='Test')
plt.xticks(ticks=x, labels=performance.keys(),
           rotation=45)
for index, value in enumerate(val_mae):
    plt.text(index - 0.4, value+0.01, str(value)[0:4])
for index, value in enumerate(test_mae):
    plt.text(index + 0.04, value + 0.01, str(value)[0:4])
plt.figure(figsize = (20, 10))
_ = plt.legend()